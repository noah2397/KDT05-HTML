{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ LSTM기반 텍스트 생성]\n",
    "- 기사의 제목을 생성 ==> 앞 부분 2개 단어 입력으로 나머지 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1] 데이터 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 모듈로딩\n",
    "import pandas as pd\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 데이터 관련 경로\n",
    "data_dir = './text/'\n",
    "filename = './text/ArticlesApril2017.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 886 entries, 0 to 885\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   abstract          22 non-null     object\n",
      " 1   articleID         886 non-null    object\n",
      " 2   articleWordCount  886 non-null    int64 \n",
      " 3   byline            886 non-null    object\n",
      " 4   documentType      886 non-null    object\n",
      " 5   headline          886 non-null    object\n",
      " 6   keywords          886 non-null    object\n",
      " 7   multimedia        886 non-null    int64 \n",
      " 8   newDesk           886 non-null    object\n",
      " 9   printPage         886 non-null    int64 \n",
      " 10  pubDate           886 non-null    object\n",
      " 11  sectionName       886 non-null    object\n",
      " 12  snippet           886 non-null    object\n",
      " 13  source            886 non-null    object\n",
      " 14  typeOfMaterial    886 non-null    object\n",
      " 15  webURL            886 non-null    object\n",
      "dtypes: int64(3), object(13)\n",
      "memory usage: 110.9+ KB\n"
     ]
    }
   ],
   "source": [
    "dataDF= pd.read_csv(filename)\n",
    "\n",
    "dataDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>716</td>\n",
       "      <td>By STEPHEN HILTNER and SUSAN LEHMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>Finding an Expansive View  of a Forgotten Peop...</td>\n",
       "      <td>['Photography', 'New York Times', 'Niger', 'Fe...</td>\n",
       "      <td>3</td>\n",
       "      <td>Insider</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-04-01 00:15:41</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>One of the largest photo displays in Times his...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/insider/nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def3237c459f24986d7c84</td>\n",
       "      <td>823</td>\n",
       "      <td>By GAIL COLLINS</td>\n",
       "      <td>article</td>\n",
       "      <td>And Now,  the Dreaded Trump Curse</td>\n",
       "      <td>['United States Politics and Government', 'Tru...</td>\n",
       "      <td>3</td>\n",
       "      <td>OpEd</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-04-01 00:23:58</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Meet the gang from under the bus.</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Op-Ed</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  abstract                 articleID  articleWordCount  \\\n",
       "0      NaN  58def1347c459f24986d7c80               716   \n",
       "1      NaN  58def3237c459f24986d7c84               823   \n",
       "\n",
       "                                byline documentType  \\\n",
       "0  By STEPHEN HILTNER and SUSAN LEHMAN      article   \n",
       "1                      By GAIL COLLINS      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Finding an Expansive View  of a Forgotten Peop...   \n",
       "1                  And Now,  the Dreaded Trump Curse   \n",
       "\n",
       "                                            keywords  multimedia  newDesk  \\\n",
       "0  ['Photography', 'New York Times', 'Niger', 'Fe...           3  Insider   \n",
       "1  ['United States Politics and Government', 'Tru...           3     OpEd   \n",
       "\n",
       "   printPage              pubDate sectionName  \\\n",
       "0          2  2017-04-01 00:15:41     Unknown   \n",
       "1         23  2017-04-01 00:23:58     Unknown   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  One of the largest photo displays in Times his...  The New York Times   \n",
       "1                  Meet the gang from under the bus.  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \n",
       "0           News  https://www.nytimes.com/2017/03/31/insider/nig...  \n",
       "1          Op-Ed  https://www.nytimes.com/2017/03/31/opinion/and...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDF.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2] 커스텀 테이서셋 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneration(Dataset):    \n",
    "\n",
    "    def __init__(self):\n",
    "        all_headlines = []\n",
    "\n",
    "        # 모든 헤드라인 텍스트 로딩\n",
    "        for filename in glob.glob(data_dir+\"*.csv\"):\n",
    "            if 'Articles' in filename:\n",
    "                article_df = pd.read_csv(filename)\n",
    "\n",
    "                # 데이터셋의 headline의 값을 all_headlines에 추가\n",
    "                all_headlines.extend(list(article_df.headline.values))\n",
    "                break\n",
    "\n",
    "        # headline 중 unknown 값 제거\n",
    "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "        \n",
    "        # 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
    "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.BOW = {}\n",
    "\n",
    "        # 모든 문장의 단어 추출해 고유번호 지정\n",
    "        for line in self.corpus:\n",
    "            for word in line.split():\n",
    "                if word not in self.BOW.keys():\n",
    "                    self.BOW[word] = len(self.BOW.keys())\n",
    "\n",
    "        # 모델의 입력으로 사용할 데이터\n",
    "        self.data = self.generate_sequence(self.corpus)\n",
    "   \n",
    "    ##  전처리 함수 \n",
    "    def clean_text(self, txt):\n",
    "        # 소문자 변환 및 특수문자 제거\n",
    "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "        return txt         \n",
    "        \n",
    "    ## 단어 순서 지정 함수     \n",
    "    def generate_sequence(self, txt):\n",
    "        seq = []\n",
    "\n",
    "        for line in txt:\n",
    "            line = line.split()\n",
    "            line_bow = [self.BOW[word] for word in line]  # 단어 => 숫자 \n",
    "\n",
    "            data=[([line_bow[i], line_bow[i+1]], line_bow[i+2])  for i in range(len(line_bow)-2)]\n",
    "            \n",
    "            seq.extend(data)\n",
    "        return seq\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        data = np.array(self.data[i][0])  \n",
    "        label = np.array(self.data[i][1]).astype(np.float32)  \n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [3] 모델 정의 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 하이퍼파라미터 \n",
    "EMBEDDING_DIM = 16\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "   ## 모델 구조 정의 \n",
    "   def __init__(self, num_embeddings):\n",
    "       super(LSTM, self).__init__()\n",
    "\n",
    "       # 임베딩층\n",
    "       self.embed = nn.Embedding( num_embeddings=num_embeddings, embedding_dim=EMBEDDING_DIM)\n",
    "       \n",
    "       # LSTM 5개층\n",
    "       self.lstm = nn.LSTM( input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, \n",
    "                            num_layers=NUM_LAYERS, batch_first=True)\n",
    "       \n",
    "       # 분류 위한 MLP층\n",
    "       self.fc1 = nn.Linear(128, num_embeddings)\n",
    "       self.fc2 = nn.Linear(num_embeddings,num_embeddings)\n",
    "\n",
    "       # 활성화 함수\n",
    "       self.relu = nn.ReLU()\n",
    "       \n",
    "\n",
    "   ## 순방향 학습 진행 함수 \n",
    "   def forward(self, x):\n",
    "       x = self.embed(x)\n",
    "\n",
    "       # LSTM 모델 예측값\n",
    "       x, _ = self.lstm(x)\n",
    "       x = torch.reshape(x, (x.shape[0], -1))\n",
    "       x = self.fc1(x)\n",
    "       x = self.relu(x)\n",
    "       x = self.fc2(x)\n",
    "\n",
    "       return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [3] 학습 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 모듈로딩\n",
    "import tqdm\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.adam import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT_DS.BOW : 2482개\n",
      "MODE DESC\n",
      "LSTM(\n",
      "  (embed): Embedding(2482, 16)\n",
      "  (lstm): LSTM(16, 64, num_layers=5, batch_first=True)\n",
      "  (fc1): Linear(in_features=128, out_features=2482, bias=True)\n",
      "  (fc2): Linear(in_features=2482, out_features=2482, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 학습관련 하이퍼파라미터 \n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "EPOCHS = 200\n",
    "\n",
    "# 데이터셋 정의\n",
    "TEXT_DS = TextGeneration()  \n",
    "DATA_LD = DataLoader(TEXT_DS, batch_size=BATCH_SIZE)\n",
    "print(f'TEXT_DS.BOW : {len(TEXT_DS.BOW)}개')\n",
    "VOCAB_SIZE = len(TEXT_DS.BOW)\n",
    "\n",
    "# 모델 정의\n",
    "MODEL = LSTM(num_embeddings=VOCAB_SIZE).to(DEVICE) \n",
    "print(f'MODE DESC\\n{MODEL}')\n",
    "\n",
    "OPTIMIZER = Adam(MODEL.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    # 학습 모드 설정 \n",
    "    MODEL.train()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # 진행 프로그래스바 출력 연동\n",
    "        iterator = tqdm.tqdm(DATA_LD)\n",
    "        \n",
    "        for data, label in iterator:\n",
    "            # 기울기 초기화\n",
    "            OPTIMIZER.zero_grad()\n",
    "\n",
    "            # 모델의 예측값\n",
    "            pred = MODEL(torch.tensor(data, dtype=torch.long).to(DEVICE))\n",
    "\n",
    "            # 정답 레이블 >>> long 텐서 반환\n",
    "            loss = nn.CrossEntropyLoss()(\n",
    "                pred, torch.tensor(label, dtype=torch.long).to(DEVICE))\n",
    "            \n",
    "            # 오차 역전파\n",
    "            loss.backward()\n",
    "            OPTIMIZER.step()\n",
    "\n",
    "            iterator.set_description(f\"epoch{epoch} loss:{loss.item()}\")\n",
    "\n",
    "    torch.save(MODEL.state_dict(), f\"lstm_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]C:\\Users\\mathn\\AppData\\Local\\Temp\\ipykernel_17300\\326276439.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = MODEL(torch.tensor(data, dtype=torch.long).to(DEVICE))\n",
      "C:\\Users\\mathn\\AppData\\Local\\Temp\\ipykernel_17300\\326276439.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred, torch.tensor(label, dtype=torch.long).to(DEVICE))\n",
      "epoch0 loss:7.37381649017334: 100%|██████████| 63/63 [00:04<00:00, 14.73it/s]  \n",
      "epoch1 loss:7.015031337738037: 100%|██████████| 63/63 [00:04<00:00, 15.71it/s] \n",
      "epoch2 loss:6.778079986572266: 100%|██████████| 63/63 [00:04<00:00, 15.61it/s] \n",
      "epoch3 loss:6.586443901062012: 100%|██████████| 63/63 [00:04<00:00, 15.45it/s] \n",
      "epoch4 loss:6.443823337554932: 100%|██████████| 63/63 [00:04<00:00, 15.43it/s] \n",
      "epoch5 loss:6.345488548278809: 100%|██████████| 63/63 [00:03<00:00, 15.81it/s] \n",
      "epoch6 loss:6.247572422027588: 100%|██████████| 63/63 [00:03<00:00, 16.43it/s] \n",
      "epoch7 loss:6.120052814483643: 100%|██████████| 63/63 [00:03<00:00, 16.30it/s] \n",
      "epoch8 loss:5.924457550048828: 100%|██████████| 63/63 [00:03<00:00, 16.09it/s] \n",
      "epoch9 loss:6.040002346038818: 100%|██████████| 63/63 [00:03<00:00, 16.09it/s] \n",
      "epoch10 loss:5.948252201080322: 100%|██████████| 63/63 [00:04<00:00, 15.44it/s] \n",
      "epoch11 loss:5.645752429962158: 100%|██████████| 63/63 [00:04<00:00, 14.87it/s] \n",
      "epoch12 loss:5.53664493560791: 100%|██████████| 63/63 [00:04<00:00, 13.60it/s]  \n",
      "epoch13 loss:5.454278469085693: 100%|██████████| 63/63 [00:04<00:00, 14.95it/s] \n",
      "epoch14 loss:5.118020534515381: 100%|██████████| 63/63 [00:04<00:00, 14.59it/s] \n",
      "epoch15 loss:5.215714454650879: 100%|██████████| 63/63 [00:04<00:00, 14.84it/s] \n",
      "epoch16 loss:5.1568498611450195: 100%|██████████| 63/63 [00:04<00:00, 14.02it/s]\n",
      "epoch17 loss:4.8009538650512695: 100%|██████████| 63/63 [00:04<00:00, 14.57it/s]\n",
      "epoch18 loss:4.614983081817627: 100%|██████████| 63/63 [00:04<00:00, 14.62it/s] \n",
      "epoch19 loss:4.646790981292725: 100%|██████████| 63/63 [00:04<00:00, 14.47it/s] \n",
      "epoch20 loss:4.573240756988525: 100%|██████████| 63/63 [00:04<00:00, 14.45it/s] \n",
      "epoch21 loss:4.459478378295898: 100%|██████████| 63/63 [00:04<00:00, 14.25it/s] \n",
      "epoch22 loss:4.622023105621338: 100%|██████████| 63/63 [00:04<00:00, 13.91it/s] \n",
      "epoch23 loss:4.698894500732422: 100%|██████████| 63/63 [00:04<00:00, 14.74it/s] \n",
      "epoch24 loss:4.6158127784729: 100%|██████████| 63/63 [00:04<00:00, 14.95it/s]   \n",
      "epoch25 loss:4.451480388641357: 100%|██████████| 63/63 [00:04<00:00, 14.01it/s] \n",
      "epoch26 loss:4.362425327301025: 100%|██████████| 63/63 [00:04<00:00, 14.43it/s] \n",
      "epoch27 loss:4.343173027038574: 100%|██████████| 63/63 [00:04<00:00, 13.67it/s] \n",
      "epoch28 loss:4.353273868560791: 100%|██████████| 63/63 [00:04<00:00, 13.45it/s] \n",
      "epoch29 loss:4.259767055511475: 100%|██████████| 63/63 [00:04<00:00, 14.36it/s] \n",
      "epoch30 loss:4.017884731292725: 100%|██████████| 63/63 [00:04<00:00, 14.00it/s] \n",
      "epoch31 loss:3.812037706375122: 100%|██████████| 63/63 [00:04<00:00, 14.01it/s] \n",
      "epoch32 loss:3.76005482673645: 100%|██████████| 63/63 [00:04<00:00, 13.93it/s]  \n",
      "epoch33 loss:3.846158266067505: 100%|██████████| 63/63 [00:04<00:00, 14.02it/s] \n",
      "epoch34 loss:3.883629322052002: 100%|██████████| 63/63 [00:04<00:00, 14.17it/s] \n",
      "epoch35 loss:3.802302598953247: 100%|██████████| 63/63 [00:04<00:00, 13.42it/s] \n",
      "epoch36 loss:3.8597347736358643: 100%|██████████| 63/63 [00:04<00:00, 13.29it/s]\n",
      "epoch37 loss:3.9913723468780518: 100%|██████████| 63/63 [00:04<00:00, 14.27it/s]\n",
      "epoch38 loss:3.9348907470703125: 100%|██████████| 63/63 [00:04<00:00, 14.18it/s]\n",
      "epoch39 loss:3.7521164417266846: 100%|██████████| 63/63 [00:04<00:00, 14.40it/s]\n",
      "epoch40 loss:3.7126526832580566: 100%|██████████| 63/63 [00:04<00:00, 14.27it/s]\n",
      "epoch41 loss:4.080096244812012: 100%|██████████| 63/63 [00:04<00:00, 14.73it/s] \n",
      "epoch42 loss:4.080567836761475: 100%|██████████| 63/63 [00:04<00:00, 14.46it/s] \n",
      "epoch43 loss:3.782073497772217: 100%|██████████| 63/63 [00:04<00:00, 14.95it/s] \n",
      "epoch44 loss:3.6005160808563232: 100%|██████████| 63/63 [00:04<00:00, 14.92it/s]\n",
      "epoch45 loss:3.4560859203338623: 100%|██████████| 63/63 [00:04<00:00, 15.00it/s]\n",
      "epoch46 loss:3.3713767528533936: 100%|██████████| 63/63 [00:04<00:00, 15.16it/s]\n",
      "epoch47 loss:3.285414934158325: 100%|██████████| 63/63 [00:04<00:00, 14.96it/s] \n",
      "epoch48 loss:3.2326197624206543: 100%|██████████| 63/63 [00:04<00:00, 15.26it/s]\n",
      "epoch49 loss:3.1239664554595947: 100%|██████████| 63/63 [00:04<00:00, 14.94it/s]\n",
      "epoch50 loss:3.0567901134490967: 100%|██████████| 63/63 [00:04<00:00, 15.07it/s]\n",
      "epoch51 loss:2.94242525100708: 100%|██████████| 63/63 [00:04<00:00, 14.92it/s]  \n",
      "epoch52 loss:2.895409345626831: 100%|██████████| 63/63 [00:04<00:00, 14.99it/s] \n",
      "epoch53 loss:2.728724479675293: 100%|██████████| 63/63 [00:04<00:00, 15.28it/s] \n",
      "epoch54 loss:2.777392864227295: 100%|██████████| 63/63 [00:04<00:00, 15.12it/s] \n",
      "epoch55 loss:2.641775131225586: 100%|██████████| 63/63 [00:04<00:00, 14.71it/s] \n",
      "epoch56 loss:2.585057258605957: 100%|██████████| 63/63 [00:04<00:00, 15.26it/s] \n",
      "epoch57 loss:2.5533602237701416: 100%|██████████| 63/63 [00:04<00:00, 15.02it/s]\n",
      "epoch58 loss:2.414917469024658: 100%|██████████| 63/63 [00:04<00:00, 15.21it/s] \n",
      "epoch59 loss:2.441868782043457: 100%|██████████| 63/63 [00:04<00:00, 15.09it/s] \n",
      "epoch60 loss:2.6169140338897705: 100%|██████████| 63/63 [00:04<00:00, 15.25it/s]\n",
      "epoch61 loss:2.4061055183410645: 100%|██████████| 63/63 [00:04<00:00, 14.72it/s]\n",
      "epoch62 loss:2.352166175842285: 100%|██████████| 63/63 [00:04<00:00, 14.98it/s] \n",
      "epoch63 loss:2.345360040664673: 100%|██████████| 63/63 [00:04<00:00, 15.03it/s] \n",
      "epoch64 loss:2.4063754081726074: 100%|██████████| 63/63 [00:04<00:00, 14.70it/s]\n",
      "epoch65 loss:2.467292308807373: 100%|██████████| 63/63 [00:04<00:00, 14.98it/s] \n",
      "epoch66 loss:2.5138871669769287: 100%|██████████| 63/63 [00:04<00:00, 15.02it/s]\n",
      "epoch67 loss:2.2125439643859863: 100%|██████████| 63/63 [00:04<00:00, 14.56it/s]\n",
      "epoch68 loss:2.0303726196289062: 100%|██████████| 63/63 [00:04<00:00, 14.68it/s]\n",
      "epoch69 loss:1.7541377544403076: 100%|██████████| 63/63 [00:04<00:00, 14.50it/s]\n",
      "epoch70 loss:1.7532265186309814: 100%|██████████| 63/63 [00:04<00:00, 14.68it/s]\n",
      "epoch71 loss:1.789823293685913: 100%|██████████| 63/63 [00:04<00:00, 13.80it/s] \n",
      "epoch72 loss:1.7372806072235107: 100%|██████████| 63/63 [00:04<00:00, 14.01it/s]\n",
      "epoch73 loss:1.5381882190704346: 100%|██████████| 63/63 [00:04<00:00, 14.90it/s]\n",
      "epoch74 loss:1.4026658535003662: 100%|██████████| 63/63 [00:04<00:00, 14.88it/s]\n",
      "epoch75 loss:1.4384485483169556: 100%|██████████| 63/63 [00:04<00:00, 14.56it/s]\n",
      "epoch76 loss:1.5784202814102173: 100%|██████████| 63/63 [00:04<00:00, 14.34it/s]\n",
      "epoch77 loss:1.5744717121124268: 100%|██████████| 63/63 [00:04<00:00, 14.59it/s]\n",
      "epoch78 loss:1.6185728311538696: 100%|██████████| 63/63 [00:04<00:00, 14.72it/s]\n",
      "epoch79 loss:1.7619396448135376: 100%|██████████| 63/63 [00:04<00:00, 14.98it/s]\n",
      "epoch80 loss:1.6887060403823853: 100%|██████████| 63/63 [00:04<00:00, 14.86it/s]\n",
      "epoch81 loss:1.5552586317062378: 100%|██████████| 63/63 [00:04<00:00, 14.93it/s]\n",
      "epoch82 loss:1.4474875926971436: 100%|██████████| 63/63 [00:04<00:00, 15.03it/s]\n",
      "epoch83 loss:1.436262845993042: 100%|██████████| 63/63 [00:04<00:00, 14.93it/s] \n",
      "epoch84 loss:1.39275324344635: 100%|██████████| 63/63 [00:04<00:00, 14.92it/s]  \n",
      "epoch85 loss:1.5156131982803345: 100%|██████████| 63/63 [00:04<00:00, 14.89it/s]\n",
      "epoch86 loss:1.5292079448699951: 100%|██████████| 63/63 [00:04<00:00, 14.71it/s]\n",
      "epoch87 loss:1.3971625566482544: 100%|██████████| 63/63 [00:04<00:00, 14.95it/s]\n",
      "epoch88 loss:1.4166560173034668: 100%|██████████| 63/63 [00:04<00:00, 14.71it/s]\n",
      "epoch89 loss:1.3336669206619263: 100%|██████████| 63/63 [00:04<00:00, 14.91it/s]\n",
      "epoch90 loss:1.1983046531677246: 100%|██████████| 63/63 [00:04<00:00, 15.00it/s]\n",
      "epoch91 loss:1.0930289030075073: 100%|██████████| 63/63 [00:04<00:00, 14.78it/s]\n",
      "epoch92 loss:1.1058319807052612: 100%|██████████| 63/63 [00:04<00:00, 14.84it/s]\n",
      "epoch93 loss:0.9829123616218567: 100%|██████████| 63/63 [00:04<00:00, 15.06it/s]\n",
      "epoch94 loss:0.9783973693847656: 100%|██████████| 63/63 [00:04<00:00, 14.98it/s]\n",
      "epoch95 loss:1.4483652114868164: 100%|██████████| 63/63 [00:04<00:00, 14.99it/s]\n",
      "epoch96 loss:1.597029685974121: 100%|██████████| 63/63 [00:04<00:00, 14.94it/s] \n",
      "epoch97 loss:1.404287576675415: 100%|██████████| 63/63 [00:04<00:00, 15.02it/s] \n",
      "epoch98 loss:1.363313913345337: 100%|██████████| 63/63 [00:04<00:00, 14.61it/s] \n",
      "epoch99 loss:1.2235866785049438: 100%|██████████| 63/63 [00:04<00:00, 14.00it/s]\n",
      "epoch100 loss:1.1422348022460938: 100%|██████████| 63/63 [00:04<00:00, 13.51it/s]\n",
      "epoch101 loss:0.992761492729187: 100%|██████████| 63/63 [00:04<00:00, 13.90it/s] \n",
      "epoch102 loss:0.9093710780143738: 100%|██████████| 63/63 [00:04<00:00, 14.48it/s]\n",
      "epoch103 loss:0.8152265548706055: 100%|██████████| 63/63 [00:04<00:00, 14.68it/s]\n",
      "epoch104 loss:0.7556988596916199: 100%|██████████| 63/63 [00:04<00:00, 14.66it/s]\n",
      "epoch105 loss:0.7639638185501099: 100%|██████████| 63/63 [00:04<00:00, 14.04it/s]\n",
      "epoch106 loss:0.7238760590553284: 100%|██████████| 63/63 [00:04<00:00, 14.19it/s]\n",
      "epoch107 loss:0.8495248556137085: 100%|██████████| 63/63 [00:04<00:00, 14.79it/s]\n",
      "epoch108 loss:0.7398623824119568: 100%|██████████| 63/63 [00:04<00:00, 14.51it/s]\n",
      "epoch109 loss:0.7791787981987: 100%|██████████| 63/63 [00:04<00:00, 14.68it/s]   \n",
      "epoch110 loss:0.5141261219978333: 100%|██████████| 63/63 [00:04<00:00, 14.42it/s]\n",
      "epoch111 loss:0.7416588068008423: 100%|██████████| 63/63 [00:04<00:00, 14.42it/s]\n",
      "epoch112 loss:0.602530837059021: 100%|██████████| 63/63 [00:04<00:00, 14.61it/s] \n",
      "epoch113 loss:0.7068003416061401: 100%|██████████| 63/63 [00:04<00:00, 13.64it/s]\n",
      "epoch114 loss:0.7256414890289307: 100%|██████████| 63/63 [00:04<00:00, 13.80it/s]\n",
      "epoch115 loss:0.5642549395561218: 100%|██████████| 63/63 [00:04<00:00, 14.09it/s]\n",
      "epoch116 loss:0.6965960264205933: 100%|██████████| 63/63 [00:04<00:00, 14.13it/s]\n",
      "epoch117 loss:0.5851849317550659: 100%|██████████| 63/63 [00:04<00:00, 14.22it/s]\n",
      "epoch118 loss:0.6836296916007996: 100%|██████████| 63/63 [00:04<00:00, 13.62it/s]\n",
      "epoch119 loss:1.467533826828003:  54%|█████▍    | 34/63 [00:02<00:02, 13.66it/s] "
     ]
    }
   ],
   "source": [
    "### 학습 진행\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 생성 \n",
    "def generate(model, BOW, string=\"finding an \", strlen=10):\n",
    "   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "   print(f\"input word: {string}\")\n",
    "\n",
    "   with torch.no_grad():\n",
    "       for p in range(strlen):\n",
    "           # 입력 문장 텐서로 변경\n",
    "           words = torch.tensor(\n",
    "               [BOW[w] for w in string.split()], dtype=torch.long).to(device)\n",
    "\n",
    "           # 입력 텐서 shape 변환\n",
    "           input_tensor = torch.unsqueeze(words[-2:], dim=0)\n",
    "           output = model(input_tensor)                                 # 모델 이용해 예측\n",
    "           output_word = (torch.argmax(output).cpu().numpy())\n",
    "           string += list(BOW.keys())[output_word]          # 문장에 예측된 단어 추가\n",
    "           string += \" \"\n",
    "\n",
    "   print(f\"predicted sentence: {string}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word: finding an \n",
      "predicted sentence: finding an expansive view of a reading central good home’ economics will \n"
     ]
    }
   ],
   "source": [
    "MODEL.load_state_dict(torch.load(\"lstm.pth\", map_location=DEVICE))\n",
    "pred = generate(MODEL, TEXT_DS.BOW)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
